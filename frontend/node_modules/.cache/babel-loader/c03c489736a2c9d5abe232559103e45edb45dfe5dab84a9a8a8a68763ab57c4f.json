{"ast":null,"code":"// import { GoogleGenerativeAI } from \"@google/generative-ai\";\n\n// Initialize Gemini AI with your API key\nconst genAI = process.env.GEMINI_API_KEY;\nexport async function getChatResponse(history, userMessage) {\n  try {\n    // Convert chat history to Gemini format\n    const formattedHistory = history.map(msg => ({\n      role: msg.role === \"user\" ? \"user\" : \"model\",\n      parts: [{\n        text: msg.content\n      }]\n    }));\n\n    // Start a chat\n    const model = genAI.getGenerativeModel({\n      model: \"gemini-pro\"\n    });\n    const chat = model.startChat({\n      history: formattedHistory,\n      generationConfig: {\n        temperature: 0.7,\n        topK: 40,\n        topP: 0.95,\n        maxOutputTokens: 1024\n      }\n    });\n\n    // Send message and get response\n    const result = await chat.sendMessage(userMessage);\n    const response = await result.response;\n    const text = response.text();\n    return text;\n  } catch (error) {\n    console.error(\"Gemini AI Error:\", error);\n    throw new Error(\"Failed to get response from Gemini AI\");\n  }\n}","map":{"version":3,"names":["genAI","process","env","GEMINI_API_KEY","getChatResponse","history","userMessage","formattedHistory","map","msg","role","parts","text","content","model","getGenerativeModel","chat","startChat","generationConfig","temperature","topK","topP","maxOutputTokens","result","sendMessage","response","error","console","Error"],"sources":["C:/Users/rkama/OneDrive/Desktop/FSD/Mini-Project/frontend/src/lib/gemini.js"],"sourcesContent":["// import { GoogleGenerativeAI } from \"@google/generative-ai\";\r\n\r\n// Initialize Gemini AI with your API key\r\nconst genAI = process.env.GEMINI_API_KEY;\r\n\r\nexport async function getChatResponse(history, userMessage) {\r\n  try {\r\n    // Convert chat history to Gemini format\r\n    const formattedHistory = history.map((msg) => ({\r\n      role: msg.role === \"user\" ? \"user\" : \"model\",\r\n      parts: [{ text: msg.content }],\r\n    }));\r\n\r\n    // Start a chat\r\n    const model = genAI.getGenerativeModel({ model: \"gemini-pro\" });\r\n    const chat = model.startChat({\r\n      history: formattedHistory,\r\n      generationConfig: {\r\n        temperature: 0.7,\r\n        topK: 40,\r\n        topP: 0.95,\r\n        maxOutputTokens: 1024,\r\n      },\r\n    });\r\n\r\n    // Send message and get response\r\n    const result = await chat.sendMessage(userMessage);\r\n    const response = await result.response;\r\n    const text = response.text();\r\n\r\n    return text;\r\n  } catch (error) {\r\n    console.error(\"Gemini AI Error:\", error);\r\n    throw new Error(\"Failed to get response from Gemini AI\");\r\n  }\r\n}\r\n"],"mappings":"AAAA;;AAEA;AACA,MAAMA,KAAK,GAAGC,OAAO,CAACC,GAAG,CAACC,cAAc;AAExC,OAAO,eAAeC,eAAeA,CAACC,OAAO,EAAEC,WAAW,EAAE;EAC1D,IAAI;IACF;IACA,MAAMC,gBAAgB,GAAGF,OAAO,CAACG,GAAG,CAAEC,GAAG,KAAM;MAC7CC,IAAI,EAAED,GAAG,CAACC,IAAI,KAAK,MAAM,GAAG,MAAM,GAAG,OAAO;MAC5CC,KAAK,EAAE,CAAC;QAAEC,IAAI,EAAEH,GAAG,CAACI;MAAQ,CAAC;IAC/B,CAAC,CAAC,CAAC;;IAEH;IACA,MAAMC,KAAK,GAAGd,KAAK,CAACe,kBAAkB,CAAC;MAAED,KAAK,EAAE;IAAa,CAAC,CAAC;IAC/D,MAAME,IAAI,GAAGF,KAAK,CAACG,SAAS,CAAC;MAC3BZ,OAAO,EAAEE,gBAAgB;MACzBW,gBAAgB,EAAE;QAChBC,WAAW,EAAE,GAAG;QAChBC,IAAI,EAAE,EAAE;QACRC,IAAI,EAAE,IAAI;QACVC,eAAe,EAAE;MACnB;IACF,CAAC,CAAC;;IAEF;IACA,MAAMC,MAAM,GAAG,MAAMP,IAAI,CAACQ,WAAW,CAAClB,WAAW,CAAC;IAClD,MAAMmB,QAAQ,GAAG,MAAMF,MAAM,CAACE,QAAQ;IACtC,MAAMb,IAAI,GAAGa,QAAQ,CAACb,IAAI,CAAC,CAAC;IAE5B,OAAOA,IAAI;EACb,CAAC,CAAC,OAAOc,KAAK,EAAE;IACdC,OAAO,CAACD,KAAK,CAAC,kBAAkB,EAAEA,KAAK,CAAC;IACxC,MAAM,IAAIE,KAAK,CAAC,uCAAuC,CAAC;EAC1D;AACF","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}